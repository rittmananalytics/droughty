{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b24b5ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lewischarlesbaker/.virtualenvs/droughty_dev_0.1.0/lib/python3.8/site-packages/snowflake/connector/options.py:96: UserWarning: You have an incompatible version of 'pyarrow' installed (7.0.0), please install a version that adheres to: 'pyarrow<6.1.0,>=6.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not enough arguments for format string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdroughty\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warehouse_target\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdroughty\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgit\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/droughty_dev_0.1.0/lib/python3.8/site-packages/droughty/warehouse_target.py:607\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_sql_from_template(query, bind_params)\n\u001b[1;32m    605\u001b[0m explore_sql \u001b[38;5;241m=\u001b[39m (query \u001b[38;5;241m%\u001b[39m bind_params)\n\u001b[0;32m--> 607\u001b[0m cube_explore_schema \u001b[38;5;241m=\u001b[39m (\u001b[43mcube_query\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcube_bind_params\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: not enough arguments for format string"
     ]
    }
   ],
   "source": [
    "import lkml as looker\n",
    "from pprint import pprint\n",
    "from google.oauth2 import service_account\n",
    "import pandas_gbq\n",
    "from contextlib import redirect_stdout\n",
    "import snowflake.connector\n",
    "from sqlalchemy import create_engine\n",
    "from snowflake.sqlalchemy import URL\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "from droughty import warehouse_target\n",
    "from droughty import config\n",
    "\n",
    "import git\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "credentials = config.service_account\n",
    "\n",
    "warehouse_name = config.warehouse_name\n",
    "lookml_project = config.project_name\n",
    "\n",
    "dimensional_inference_status = warehouse_target.dimensional_inference\n",
    "sql = warehouse_target.warehouse_schema\n",
    "explore_sql = warehouse_target.cube_explore_schema\n",
    "\n",
    "if warehouse_name == 'big_query':\n",
    "\n",
    "    # Run a Standard SQL query with the project set explicitly\n",
    "    project_id = lookml_project\n",
    "    df = pandas.read_gbq(sql, dialect='standard', project_id=lookml_project, credentials=credentials)\n",
    "\n",
    "    df['description'] = df['description'].fillna('not available')\n",
    "\n",
    "    df1 = df[['table_name','column_name','data_type','description']]\n",
    "\n",
    "    df1['data_type'] = df1['data_type'].str.replace('TIMESTAMP','timestamp')\n",
    "    df1['data_type'] = df1['data_type'].str.replace('DATE','date')\n",
    "    df1['data_type'] = df1['data_type'].str.replace('INT64','number')\n",
    "    df1['data_type'] = df1['data_type'].str.replace('FLOAT64','number')\n",
    "    df1['data_type'] = df1['data_type'].str.replace('NUMERIC','number')\n",
    "    df1['data_type'] = df1['data_type'].str.replace('STRING','string')\n",
    "    df1['data_type'] = df1['data_type'].str.replace('BOOL','yesno')\n",
    "\n",
    "    df2 = df1\n",
    "\n",
    "    explore_df = pandas.read_gbq(explore_sql, dialect='standard', project_id=lookml_project, credentials=credentials)\n",
    "\n",
    "    explore_df_2 = explore_df[['pk_table_name', 'pk_column_name','fk_table_name','fk_column_name','true_relationship']]\n",
    "\n",
    "    pk_table_name_df = explore_df[['pk_table_name']]\n",
    "\n",
    "    duplicate_explore_rows = pk_table_name_df[pk_table_name_df.duplicated(['pk_table_name'])]\n",
    "\n",
    "    distinct_duplicate_explore_rows = duplicate_explore_rows['pk_table_name'].drop_duplicates().to_list()\n",
    "\n",
    "    df4 = {n: grp.loc[n].to_dict('index')\n",
    "        \n",
    "    for n, grp in explore_df.set_index(['pk_table_name', 'pk_column_name','fk_table_name','fk_column_name','true_relationship']).groupby(level='pk_table_name')}\n",
    "\n",
    "    d2 = df4\n",
    "\n",
    "    def recur_dictify(frame):\n",
    "        if len(frame.columns) == 1:\n",
    "            if frame.values.size == 1: return frame.values[0][0]\n",
    "            return frame.values.squeeze()\n",
    "        grouped = frame.groupby(frame.columns[0])\n",
    "        d = {k: recur_dictify(g.iloc[:,1:]) for k,g in grouped}\n",
    "        return d\n",
    "\n",
    "elif warehouse_name == 'snowflake': \n",
    "\n",
    "    url = URL(\n",
    "\n",
    "        account = config.snowflake_account,\n",
    "        user =  config.snowflake_user,\n",
    "        schema =  config.snowflake_schema,\n",
    "        database =  config.snowflake_database,\n",
    "        password =  config.snowflake_password,\n",
    "        warehouse= config.snowflake_warehouse,\n",
    "        role =  config.snowflake_role\n",
    "\n",
    "    )\n",
    "\n",
    "    engine = create_engine(url)\n",
    "\n",
    "    connection = engine.connect()\n",
    "\n",
    "    explore_df = pd.read_sql(explore_sql, connection)\n",
    "\n",
    "    explore_df.drop_duplicates(keep=False, inplace=True)\n",
    "\n",
    "    explore_df['parent_table_name'] = explore_df['parent_table_name'].str.lower()\n",
    "    explore_df['pk_table_name'] = explore_df['pk_table_name'].str.lower()\n",
    "    explore_df['pk_column_name'] = explore_df['pk_column_name'].str.lower()\n",
    "    explore_df['fk_table_name'] = explore_df['fk_table_name'].str.lower()\n",
    "    explore_df['fk_column_name'] = explore_df['fk_column_name'].str.lower()\n",
    "\n",
    "    explore_df_2 = explore_df[['parent_table_name','pk_table_name', 'pk_column_name','fk_table_name','fk_column_name','true_relationship']]\n",
    "\n",
    "    pk_table_name_df = explore_df[['pk_table_name']]\n",
    "\n",
    "    duplicate_explore_rows = pk_table_name_df[pk_table_name_df.duplicated(['pk_table_name'])]\n",
    "\n",
    "    distinct_duplicate_explore_rows = duplicate_explore_rows['pk_table_name'].drop_duplicates().to_list()\n",
    "\n",
    "    connection.close()\n",
    "    engine.dispose()\n",
    "\n",
    "    df4 = {n: grp.loc[n].to_dict('index')\n",
    "        \n",
    "    for n, grp in explore_df.set_index(['parent_table_name','pk_table_name', 'pk_column_name','fk_table_name','fk_column_name','true_relationship']).groupby(level='parent_table_name')}\n",
    "\n",
    "    d2 = df4\n",
    "\n",
    "    def recur_dictify(frame):\n",
    "        if len(frame.columns) == 1:\n",
    "            if frame.values.size == 1: return frame.values[0][0]\n",
    "            return frame.values.squeeze()\n",
    "        grouped = frame.groupby(frame.columns[0])\n",
    "        d = {k: recur_dictify(g.iloc[:,1:]) for k,g in grouped}\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c526873",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'explore_sql' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mexplore_sql\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'explore_sql' is not defined"
     ]
    }
   ],
   "source": [
    "explore_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4028c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from google.oauth2 import service_account\n",
    "import pandas_gbq\n",
    "from contextlib import redirect_stdout\n",
    "import snowflake.connector\n",
    "from sqlalchemy import create_engine\n",
    "from snowflake.sqlalchemy import URL\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import yaml\n",
    "import git\n",
    "\n",
    "import droughty.cube_parser.cube as cube\n",
    "\n",
    "from droughty.cube_base_dict import d1\n",
    "from droughty.cube_explore_dict import d2\n",
    "from droughty.cube_explore_dict import distinct_duplicate_explore_rows\n",
    "from droughty.config import schema_name\n",
    "\n",
    "def get_all_values(nested_dictionary,explore_dictionary):\n",
    "    \n",
    "    for key,value in nested_dictionary.items():\n",
    "    \n",
    "        for explore_key, explore_value in explore_dictionary.items():\n",
    "\n",
    "            if explore_key in key:\n",
    "\n",
    "                explore = {\n",
    "\n",
    "\n",
    "                    \"cube\": key,\n",
    "                    \"sql\": \"select * from\"+\" \"+schema_name+\".\"+key\n",
    "\n",
    "                }\n",
    "\n",
    "\n",
    "                yield(cube.dump(explore))\n",
    "        \n",
    "\n",
    "                for key1, value1 in explore_value.items():  \n",
    "                    \n",
    "                    join_start = \"joins: {\"\n",
    "                    \n",
    "\n",
    "                    join = {\n",
    "\n",
    "\n",
    "                        \"joins\": \n",
    "                        [\n",
    "                        \n",
    "                        {\n",
    "                        \"relationship\": \"belongsTo\",\n",
    "                        \"sql\": \"${CUBE.\"+key1[0]+\"}\"+\" = \"+\"${\"+key1[1]+\".\"+key1[2]+\"}\",\n",
    "                        \"name\": key1[1]\n",
    "\n",
    "                        }\n",
    "                            \n",
    "                        ]\n",
    "\n",
    "                    }\n",
    "                    \n",
    "                    join_end = \"},\"                 \n",
    "                    \n",
    "\n",
    "                    yield(join_start)\n",
    "\n",
    "                    yield(cube.dump(join))\n",
    "\n",
    "                    yield(join_end)\n",
    "\n",
    "        \n",
    "nested_dictionary = d1\n",
    "explore_dictionary = d2\n",
    "\n",
    "get_all_values(nested_dictionary,explore_dictionary)\n",
    "\n",
    "\n",
    "for value in get_all_values(nested_dictionary,explore_dictionary):\n",
    "\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c9bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from droughty.cube_base_dict import d1\n",
    "from droughty.cube_explore_dict import d2\n",
    "from droughty.config import schema_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b2583",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_dictionary = d1\n",
    "explore_dictionary = d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b159ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b577879",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from google.oauth2 import service_account\n",
    "import pandas_gbq\n",
    "from contextlib import redirect_stdout\n",
    "import snowflake.connector\n",
    "from sqlalchemy import create_engine\n",
    "from snowflake.sqlalchemy import URL\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import yaml\n",
    "import git\n",
    "\n",
    "import droughty.cube_parser.cube as cube\n",
    "\n",
    "from droughty.cube_base_dict import d1\n",
    "from droughty.cube_explore_dict import d2\n",
    "from droughty.cube_explore_dict import distinct_duplicate_explore_rows\n",
    "from droughty.config import schema_name\n",
    "\n",
    "def get_all_values(nested_dictionary,explore_dictionary):\n",
    "    \n",
    "    for key,value in nested_dictionary.items():\n",
    "    \n",
    "        for explore_key, explore_value in explore_dictionary.items():\n",
    "\n",
    "            if explore_key in key:\n",
    "\n",
    "                explore = {\n",
    "\n",
    "\n",
    "                    \"cube\": key,\n",
    "                    \"sql\": \"select * from\"+\" \"+schema_name+\".\"+key,\n",
    "                    \"joins \": \"{\"\n",
    "\n",
    "                }\n",
    "\n",
    "\n",
    "                yield(cube.dump(explore))\n",
    "        \n",
    "\n",
    "                for key1, value1 in explore_value.items(): \n",
    "\n",
    "                    \n",
    "\n",
    "                    join = {\n",
    "\n",
    "\n",
    "                        \"joins\": \n",
    "                        [\n",
    "                        \n",
    "                        {\n",
    "                        \"relationship\": \"belongsTo\",\n",
    "                        \"sql\": \"${CUBE.\"+key1[0]+\"}\"+\" = \"+\"${\"+key1[1]+\".\"+key1[2]+\"}\",\n",
    "                        \"name\": key1[1]\n",
    "\n",
    "                        }\n",
    "                            \n",
    "                        ]\n",
    "\n",
    "                    }\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    yield(cube.dump(join))\n",
    "\n",
    "\n",
    "        \n",
    "nested_dictionary = d1\n",
    "explore_dictionary = d2\n",
    "\n",
    "get_all_values(nested_dictionary,explore_dictionary)\n",
    "\n",
    "\n",
    "for value in get_all_values(nested_dictionary,explore_dictionary):\n",
    "\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f559d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from google.oauth2 import service_account\n",
    "import pandas_gbq\n",
    "from contextlib import redirect_stdout\n",
    "import snowflake.connector\n",
    "from sqlalchemy import create_engine\n",
    "from snowflake.sqlalchemy import URL\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import yaml\n",
    "import git\n",
    "\n",
    "import droughty.cube_parser.cube as cube\n",
    "\n",
    "from droughty.cube_base_dict import d1\n",
    "from droughty.cube_explore_dict import d2\n",
    "from droughty.cube_explore_dict import distinct_duplicate_explore_rows\n",
    "from droughty.config import schema_name\n",
    "\n",
    "def get_all_values(nested_dictionary,explore_dictionary):\n",
    "    \n",
    "    for key,value in nested_dictionary.items():\n",
    "    \n",
    "        for explore_key, explore_value in explore_dictionary.items():\n",
    "\n",
    "            if explore_key in key:\n",
    "\n",
    "                explore = {\n",
    "\n",
    "\n",
    "                    \"cube\": key,\n",
    "                    \"sql\": \"select * from\"+\" \"+schema_name+\".\"+key,\n",
    "                    \"joins \": \"{\"\n",
    "\n",
    "                }\n",
    "\n",
    "\n",
    "                yield(cube.dump(explore))\n",
    "        \n",
    "\n",
    "                for key1, value1 in explore_value.items(): \n",
    "\n",
    "                    \n",
    "\n",
    "                    join = {\n",
    "\n",
    "\n",
    "                        \"joins\": \n",
    "                        [\n",
    "                        \n",
    "                        {\n",
    "                        \"relationship\": key1[3],\n",
    "                        \"sql\": \"${CUBE.\"+key1[0]+\"}\"+\" = \"+\"${\"+key1[1]+\".\"+key1[2]+\"}\",\n",
    "                        \"name\": key1[1]\n",
    "\n",
    "                        }\n",
    "                            \n",
    "                        ]\n",
    "\n",
    "                    }\n",
    "                    \n",
    "                    \n",
    "\n",
    "                    yield(cube.dump(join))\n",
    "                    \n",
    "                    join_end = \"},\"                 \n",
    "\n",
    "                yield(join_end)\n",
    "                \n",
    "                        \n",
    "                dimension_start = \"dimensions: {\"\n",
    "\n",
    "                yield(dimension_start)\n",
    "\n",
    "\n",
    "                for key, value in value.items():\n",
    "                    \n",
    "                    if \"pk\" not in key[0] and \"number\" not in key [1]:  \n",
    "                    \n",
    "                        dimension = {\n",
    "\n",
    "\n",
    "                            \"dimension\": {\n",
    "                            \"sql\": key[0],\n",
    "                            \"type\": key[1],\n",
    "                            \"name\": key[0]\n",
    "\n",
    "                            }\n",
    "\n",
    "                        }\n",
    "                    \n",
    "                        yield(cube.dump(dimension))\n",
    "                        \n",
    "                    elif \"pk\" in key[0]:\n",
    "\n",
    "                        dimension = {\n",
    "\n",
    "\n",
    "                            \"dimension\": {\n",
    "                                \"primaryKey\": \"true\",\n",
    "                                \"type\": key[1],\n",
    "                                \"sql\": key[0],\n",
    "                                \"name\": key[0],\n",
    "                                \"description\": key[2]\n",
    "\n",
    "                            }\n",
    "                        }\n",
    "\n",
    "                        yield(cube.dump(dimension))\n",
    "                    \n",
    "                for key,value in nested_dictionary.items():\n",
    "\n",
    "                    closing_syntax = \"}});\"\n",
    "\n",
    "                yield (closing_syntax)\n",
    "\n",
    "\n",
    "        \n",
    "nested_dictionary = d1\n",
    "explore_dictionary = d2\n",
    "\n",
    "get_all_values(nested_dictionary,explore_dictionary)\n",
    "\n",
    "\n",
    "for value in get_all_values(nested_dictionary,explore_dictionary):\n",
    "\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35fc753",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdfc177",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"SELECT name FROM `bigquery-public-data.usa_names.usa_1910_2013` \"\n",
    "    'WHERE state = \"TX\" '\n",
    "    \"LIMIT 100\"\n",
    ")\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    # Location must match that of the dataset(s) referenced in the query.\n",
    "    location=\"US\",\n",
    ")  # API request - starts the query\n",
    "\n",
    "results = query_job.result()  # Wait for query to complete.\n",
    "print(\"Got {} rows.\".format(results.total_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08fb834",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query(query, dialect='standard', project_id=lookml_project, credentials=credentials)\n",
    "client = bigquery.Client() \n",
    "    \n",
    "results = query_job.result()  # Wait for query to complete.\n",
    "\n",
    "print(\"Got {} rows.\".format(results.total_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd66bd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "\n",
    "query = (\n",
    "    \"SELECT name FROM `bigquery-public-data.usa_names.usa_1910_2013` \"\n",
    "    'WHERE state = \"TX\" '\n",
    "    \"LIMIT 100\"\n",
    ")\n",
    "query_job = client.query(query, dialect='standard', project_id=lookml_project, credentials=credentials)\n",
    "\n",
    "results = query_job.result()  # Wait for query to complete.\n",
    "print(\"Got {} rows.\".format(results.total_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d6853f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not enough arguments for format string",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdroughty\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warehouse_target\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdroughty\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgit\u001b[39;00m\n",
      "File \u001b[0;32m~/.virtualenvs/droughty_dev_0.1.0/lib/python3.8/site-packages/droughty/warehouse_target.py:607\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_sql_from_template(query, bind_params)\n\u001b[1;32m    605\u001b[0m explore_sql \u001b[38;5;241m=\u001b[39m (query \u001b[38;5;241m%\u001b[39m bind_params)\n\u001b[0;32m--> 607\u001b[0m cube_explore_schema \u001b[38;5;241m=\u001b[39m (\u001b[43mcube_query\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcube_bind_params\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: not enough arguments for format string"
     ]
    }
   ],
   "source": [
    "import lkml as looker\n",
    "from pprint import pprint\n",
    "from google.oauth2 import service_account\n",
    "import pandas_gbq\n",
    "from contextlib import redirect_stdout\n",
    "import snowflake.connector\n",
    "from sqlalchemy import create_engine\n",
    "from snowflake.sqlalchemy import URL\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "from droughty import warehouse_target\n",
    "from droughty import config\n",
    "\n",
    "import git\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "credentials = config.service_account\n",
    "\n",
    "warehouse_name = config.warehouse_name\n",
    "lookml_project = config.project_name\n",
    "\n",
    "dimensional_inference_status = warehouse_target.dimensional_inference\n",
    "sql = warehouse_target.warehouse_schema\n",
    "explore_sql = warehouse_target.cube_explore_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feba2e95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
